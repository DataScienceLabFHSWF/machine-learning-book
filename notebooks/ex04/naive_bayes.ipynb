{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c02262e5",
   "metadata": {},
   "source": [
    "# Applications of the Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33406a",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "The _Naive Bayes_ classifier is often used when it comes to the classification of textual data but can also be used for any other classification task. The underlying math comes from the _Bayes theorem_ which describes the probability of an event based on a _prior_. This prior represents the knowledge of different conditions and thus often allows for a more accurate prediction.  \n",
    "As we will use the classifier on text data, we first take a quick look/recap at useful preprocessing techniques for _Natural Language Processing_ (NLP) as you will need these to solve the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe19cb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3eb76",
   "metadata": {},
   "source": [
    "### Remove punctuation\n",
    "#### List comprehension\n",
    "There are different techniques to remove punctuation from our data. The first uses the _string_ library and a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "23f4759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['T',\n",
       " 'h',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'i',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 's',\n",
       " 'h',\n",
       " 'o',\n",
       " 'w',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " 'f',\n",
       " ' ',\n",
       " 's',\n",
       " 'o',\n",
       " 'm',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'l',\n",
       " 'e',\n",
       " 'v',\n",
       " 'a',\n",
       " 'n',\n",
       " 't',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'e',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'c',\n",
       " 'e',\n",
       " 's',\n",
       " 's',\n",
       " 'i',\n",
       " 'n',\n",
       " 'g',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'c',\n",
       " 'h',\n",
       " 'n',\n",
       " 'i',\n",
       " 'q',\n",
       " 'u',\n",
       " 'e',\n",
       " 's',\n",
       " ' ',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " ' ',\n",
       " 'N',\n",
       " 'L',\n",
       " 'P',\n",
       " ' ',\n",
       " 'p',\n",
       " 'r',\n",
       " 'o',\n",
       " 'b',\n",
       " 'l',\n",
       " 'e',\n",
       " 'm',\n",
       " 's',\n",
       " ' ',\n",
       " 'T',\n",
       " 'h',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'c',\n",
       " 'l',\n",
       " 'u',\n",
       " 'd',\n",
       " 'e',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'm',\n",
       " 'o',\n",
       " 'v',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 'p',\n",
       " 'u',\n",
       " 'n',\n",
       " 'c',\n",
       " 't',\n",
       " 'u',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'c',\n",
       " 'o',\n",
       " 'n',\n",
       " 'v',\n",
       " 'e',\n",
       " 'r',\n",
       " 's',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " ' ',\n",
       " 't',\n",
       " 'o',\n",
       " ' ',\n",
       " 'l',\n",
       " 'o',\n",
       " 'w',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'c',\n",
       " 'a',\n",
       " 's',\n",
       " 'e',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 'w',\n",
       " 'e',\n",
       " 'l',\n",
       " 'l',\n",
       " ' ',\n",
       " 'a',\n",
       " 's',\n",
       " ' ',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'm',\n",
       " 'o',\n",
       " 'v',\n",
       " 'a',\n",
       " 'l',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 's',\n",
       " 't',\n",
       " 'o',\n",
       " 'p',\n",
       " 'w',\n",
       " 'o',\n",
       " 'r',\n",
       " 'd',\n",
       " 's']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)\n",
    "\n",
    "s = (\"This is a test text to show off some of the relevant preprocessing techniques for NLP problems. These include the removal of punctuation, the conversion to lower case as well as the removal of stopwords.\")\n",
    "\n",
    "s_wo_punct = [c for c in s if c not in string.punctuation]\n",
    "s_wo_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8ab802bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test text to show off some of the relevant preprocessing techniques for NLP problems These include the removal of punctuation the conversion to lower case as well as the removal of stopwords'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_wo_punct = \"\".join(s_wo_punct)\n",
    "s_wo_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f58e3",
   "metadata": {},
   "source": [
    "#### Regular expression\n",
    "The second approach uses _regular expressions_ to find specific characters and then substitute those characters with an empty char. You are not limited to use string.punctuation but can define any char you want to be deleted from the string (see commented line in the code below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "588075a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a test text to show off some of the relevant preprocessing techniques for NLP problems These include the removal of punctuation the conversion to lower case as well as the removal of stopwords\n",
      "This is a test text to show off some of the relevant preprocessing techniques for NLP problems These include the removal of punctuation the conversion to lower case as well as the removal of stopwords\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s_wo_punct2 = re.sub(\"[.,!?:;-='...\\\"@#_]\", \"\", s)\n",
    "s_wo_punct3 = re.sub(f\"[{string.punctuation}]\", \"\", s)\n",
    "\n",
    "print(s_wo_punct2)\n",
    "print(s_wo_punct3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfa6aba",
   "metadata": {},
   "source": [
    "### Lower case\n",
    "Transform the text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bf3c861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test text to show off some of the relevant preprocessing techniques for nlp problems these include the removal of punctuation the conversion to lower case as well as the removal of stopwords'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_wo_punct_lower = s_wo_punct.lower()\n",
    "s_wo_punct_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb8818",
   "metadata": {},
   "source": [
    "### Remove stopwords\n",
    "As text classification does not usually rely on a deep understanding of the underlying text, the added value of pronouns, articles and prepositions oftentimes diminishes for these kind of tasks. They are thus entirely removed from the text corpus to reduce the dimensionality of the input data.  \n",
    "We use the python NLP package __NLTK__ which requires you to download the stopwords if you use it for the first time. Subsequent usage of the package will not require you to refetch these files every time. The stopwords are designed for different languages as can be seen in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9e35e877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'da√ü', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'f√ºr', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'k√∂nnen', 'k√∂nnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', '√ºber', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'w√§hrend', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'w√ºrde', 'w√ºrden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# You have to download stopwords Package to execute this command\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "print(stopwords.words('german'))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3542b95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test text show relevant preprocessing techniques nlp problems include removal punctuation conversion lower case well removal stopwords'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_wo_punct_lower_nosw = \" \".join([w for w in s_wo_punct_lower.split() if w not in stopwords.words(\"english\")])\n",
    "\n",
    "s_wo_punct_lower_nosw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904862f3",
   "metadata": {},
   "source": [
    "### Count Vectorizer\n",
    "A _Count Vectorizer_ is used to generate a representation of the underlying text in terms of the frequency of all words in the corpus. These features (__X__ in this case) can then be used to train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c0ebb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['another' 'boring' 'exercise' 'first' 'four' 'fourth' 'is' 'not' 'online'\n",
      " 'the' 'this' 'want' 'yet']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sample_data = ['This is the fourth exercise.','This exercise is not online yet','Exercise four is boring, I want another exercise','Is this the first exercise?']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sample_data)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72d8280",
   "metadata": {},
   "source": [
    "Here the transformed input gets stored into a dataframe. _X_ could also directly be used for training purposes (via model.fit(X))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "39183386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 1 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 1 1 1 0 1 0 1]\n",
      " [1 1 2 0 1 0 1 0 0 0 0 1 0]\n",
      " [0 0 1 1 0 0 1 0 0 1 1 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>boring</th>\n",
       "      <th>exercise</th>\n",
       "      <th>first</th>\n",
       "      <th>four</th>\n",
       "      <th>fourth</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>online</th>\n",
       "      <th>the</th>\n",
       "      <th>this</th>\n",
       "      <th>want</th>\n",
       "      <th>yet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  boring  exercise  first  four  fourth  is  not  online  the  this  \\\n",
       "0        0       0         1      0     0       1   1    0       0    1     1   \n",
       "1        0       0         1      0     0       0   1    1       1    0     1   \n",
       "2        1       1         2      0     1       0   1    0       0    0     0   \n",
       "3        0       0         1      1     0       0   1    0       0    1     1   \n",
       "\n",
       "   want  yet  \n",
       "0     0    0  \n",
       "1     0    1  \n",
       "2     1    0  \n",
       "3     0    0  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.toarray())\n",
    "\n",
    "df2 = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120279ea",
   "metadata": {},
   "source": [
    "### Naive Bayes classifier\n",
    "In order to use the classifier you first need to import it. There are three different variants preimplemented by sklearn, namely the _MultinomialNB_ which is used for the classification of text data, the _CategoricalNB_ which handles categorical data and the _GaussianNB_ for continuous features. In this exercise you will get to apply the two former types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7e7880ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, CategoricalNB\n",
    "\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "naive_bayes.fit(X,[0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc116988",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "As written in the accompanying PDF, please download the spam dataset (\"emails.csv\") from __[here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/naive_bayes)__ and load it into a pandas Dataframe. Get familiar with the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee19ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33075f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d15fe",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Visualize different aspects of the dataset (e.g. class distribution, text length of the different entries) by using matplotlib or seaborn. The text length of each sample should be stored in an extra column called _length_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f66b950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40f722b6",
   "metadata": {},
   "source": [
    "Get length for each text sample and store them in column _length_ of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704028c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860201af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb0eef61",
   "metadata": {},
   "source": [
    "Plot the distribution of lengths for _spam_ and _non-spam_ samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ddf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c467b870",
   "metadata": {},
   "source": [
    "Display the shortest (and longest) message that are stored in this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5865cb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "790750c1",
   "metadata": {},
   "source": [
    "Visualize the most frequent words (for spam and non-spam texts) with the help of the package _wordcloud_. Do you notice any meaningful differences between these two wordclouds? What is the problem with some of the frequent words (for both cases) and how would you rate the added value of these problematic words when it comes to actually training a classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec117f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47d9299",
   "metadata": {},
   "source": [
    "Calculate the class distribution between spam and non-spam data in percent and then use a barplot (or countplot if you use seaborn) to present it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588b8ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd9d5824",
   "metadata": {},
   "source": [
    "### Preprocess dataset\n",
    "Define your functions for text cleaning here and then preprocess the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994d700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3170eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e710fc4",
   "metadata": {},
   "source": [
    "Now that we have cleaned our sample texts we can use the dataset to perform the training and test procedure. Use the _CountVectorizer_ to generate the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0ade8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82abf434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fde3fb0d",
   "metadata": {},
   "source": [
    "### Training\n",
    "As we are dealing with features generated from text data, we use the _sklearn.naive_bayes.MultinomialNB_ as our underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf7e23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05791d8c",
   "metadata": {},
   "source": [
    "Performance on test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70ca26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c3e6464",
   "metadata": {},
   "source": [
    "Performance on own text samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7d7fa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e9b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e826ab36",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82add85c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197c77f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "505efd98",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task we will analyze the \"flu.csv\" dataset which can be downloaded __[here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/naive_bayes)__. It is a very small toy dataset to showcase the encoding of categorical features as well as the usage of another variant of the Naive Bayes classifier.\n",
    "\n",
    "### Preprocessing\n",
    "Get familiar with the dataset. The goal is to predict whether or not a person has the flu.  \n",
    "What are the feature columns and what is the target column in this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483202da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df82e807",
   "metadata": {},
   "source": [
    "Use the LabelEncoder (sklearn.preprocessing.LabelEncoder()) to encode the data in the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336513b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8343d2a",
   "metadata": {},
   "source": [
    "Now use the encoded features (f1 to f4) to build a new dataframe for the training of our classifier, the _zip()_ function might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5701065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2a513a5",
   "metadata": {},
   "source": [
    "### Training\n",
    "We do not use the _MultinomialNB_ but instead _sklearn.naive_bayes.CategoricalNB_ as we have to deal with categorical data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a925d439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "558fc000",
   "metadata": {},
   "source": [
    "Generate a few input samples to feed into the classifier and print the predictions as well as the predicted probabilities for each target class (_model.predict_proba()_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788989b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "568ce6a90abe48cd4c71813e2e18d608b5934a77a079188d46a97b4cb4032653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
