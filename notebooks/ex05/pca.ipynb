{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c718a5c1",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c516f7e",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca4fdbd",
   "metadata": {},
   "source": [
    "This notebook deals with the understanding and usage of the ___Principal Component Analysis (PCA)___ on different datasets. PCA is a model-based way to reduce the amount of \"uninformative\" or redundant information, meaning that it tries to reduce the amount of individual features while trying to lose as little information as possible.\n",
    "The newly obtained features from the PCA can then be used instead of the (higher-dimensional) original feature vector.\n",
    "\n",
    "__\"The key idea here is to replace redundant features with a few new features that adequately summarize information contained in the original feature space.\"__\n",
    "\n",
    "To gain some understanding we will first take a look at how to \"manually\" extract the principal components to then use tools provided by scikit-learn to achieve the same goal. Most of the information and content in _Task 1_ is taken from LINK and packed into this notebook to get a better overview as well as to have all the information in one place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f3e49",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "#### Manual and Step-by-step\n",
    "First we are going to load, split and standardize an example dataset (\"wine.data\", __[download from here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/pca)__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fd429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"wine.data\")\n",
    "\n",
    "df.columns = ['Class label', 'Alcohol', 'Malic acid', 'Ash',\n",
    "                   'Alcalinity of ash', 'Magnesium', 'Total phenols',\n",
    "                   'Flavanoids', 'Nonflavanoid phenols', 'Proanthocyanins',\n",
    "                   'Color intensity', 'Hue',\n",
    "                   'OD280/OD315 of diluted wines', 'Proline']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20843347",
   "metadata": {},
   "source": [
    "Now we use sklearn train_test_split() to split our dataset into training and test data (80/20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd14105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.iloc[:,1:].values, df.iloc[:,0].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcf540",
   "metadata": {},
   "source": [
    "Standardize data with sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af2d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "X_train_std = std_scaler.fit_transform(X_train)\n",
    "X_test_std = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c869643",
   "metadata": {},
   "source": [
    "We can extract the principal components by performing an eigendecomposition on the covariance matrix of our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d039a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigenvals, eigenvecs = np.linalg.eigh(cov_mat)\n",
    "\n",
    "print('\\nEigenvalues \\n%s' % eigenvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8adf66",
   "metadata": {},
   "source": [
    "Then we use these eigenvalues to calculate the amount of variance each of these components explains. Here we first calculate the total amount of variance by summing up all eigenvalues to then normalize each individual value by this sum. Additionally we track the cumulative explained variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_total = sum(eigenvals)\n",
    "var_explained = [(i /var_total) for i in sorted(eigenvals, reverse=True)]\n",
    "cum_var_explained = np.cumsum(var_explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a453d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "plt.bar(range(1, len(eigenvals)+1), var_explained, alpha=0.5, align='center',\n",
    "        label='Explained variance per component')\n",
    "plt.step(range(1, len(eigenvals)+1), cum_var_explained, where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33cfa78",
   "metadata": {},
   "source": [
    "##### Feature transformation\n",
    "In order to use these newly aquired principal components as features we need to perform a base transformation where we use the PCs as new axes in our coordinate system. This will lead to unrecognizable data as the values of the original features are transformed. \n",
    "Below we first combine eigenvalues and eigenvectors into a list of tuples (eigen_pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "# eigenvectors stored as columns of a matrix -> [:, i]\n",
    "eigen_pairs = [(np.abs(eigenvals[i]), eigenvecs[:, i])\n",
    "               for i in range(len(eigenvals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)\n",
    "\n",
    "eigen_pairs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989806b9",
   "metadata": {},
   "source": [
    "This list of eigen_pairs has ofc. 13 elements, one for each principal component. Each of the elements of this list are tuples, and each tuple contains the eigenvalue as well as the eigenvector coordinates for each dimension. As PCA is often used for dimensionality reduction lets assume we want only to use the first two principal components (the ones with the highest explained variance as we want to capture as much underlying information as possible).\n",
    "\n",
    "__Exercise:__ How much of the original variance was captured by the first two principal components? How much additional explained variance is provided by the third PC?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aac608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe179d2",
   "metadata": {},
   "source": [
    "As mentioned above we only want to use the first two PCs of the 13 that were derived. These two components capture more than half of the explained variance of the underlying original dataset and could thus be a suitable choice to perform dimension reduction while simultaneously minimizing the loss of information. We use _numpy.hstack_ to create a new matrix with two columns that contains all coordinate values for the first two eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis],\n",
    "               eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557f7e7e",
   "metadata": {},
   "source": [
    "__Exercise:__ Use the code from above to create a matrix _w2_ from the coordinates of the four most informative components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21997ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b984720",
   "metadata": {},
   "source": [
    "Now that we have our matrix _w_ we can transform our data by taking the dot product of the original data and the PCA-column vectors, projecting the original data onto the PCA axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8249f8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train_std[0]: {}\\n\".format(X_train_std[0]))\n",
    "print(\"X_train_std[0].dot(w): {}\".format(X_train_std[0].dot(w)))\n",
    "\n",
    "X_train_pca = X_train_std.dot(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a0873",
   "metadata": {},
   "source": [
    "When plotting the results we can see that the first two PCs already achieve quite good clustering, even though they "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ce06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train == l, 0], \n",
    "                X_train_pca[y_train == l, 1], \n",
    "                c=c, label=l, marker=m)\n",
    "\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5fbb08",
   "metadata": {},
   "source": [
    "__Exercise:__ Now take only the best and the worst PCs and use them as basis for the feature transformation. How does the resulting scatter plot differ from the one shown above? What's the reason behind this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b534e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f586a8c",
   "metadata": {},
   "source": [
    "### PCA with scikit-learn\n",
    "Of course you do not always have to perform these tasks by hand as there is already a scikit-learn implementation for the PCA. This makes it more comfortable to use and takes care of the underlying math. To perform the same operations as above we just have to do the following when utilizing sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba329afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c00de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_, alpha=0.5, align='center',\n",
    "        label='Explained variance per component')\n",
    "plt.step(range(1, len(pca.explained_variance_ratio_)+1), np.cumsum(pca.explained_variance_ratio_), where='mid',\n",
    "         label='Cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bd4967",
   "metadata": {},
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb8c84",
   "metadata": {},
   "source": [
    "In this task you will take a closer look at the _Iris_ dataset (\"IRIS.csv\", __[download from here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/pca)__) which lists different attributes of the Iris flower, taken from various samples. The goal here is to predict the correct species by applying and comparing different machine learning algorithms of your choice.  \n",
    "Perform PCA on the data and take a look at the _explained variance_ in order to get the best trade-off between dimensionality-reduction and information loss. Train and test every model you consider on the PCA-transformed and the original data to see the impact of the aforementioned loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248c2d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e6fa8a5",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "Plot the distribution of the different features in the dataset. What kind of scaling would you choose for the different attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb4b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5d93a40",
   "metadata": {},
   "source": [
    "Use the _seaborn_ package to plot a heatmap of the correlation matrix of the dataframe (df.corr()). Which attributes are highly correlated, which feature sticks out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10584fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fe82e27",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "Preprocess the data here (label encoding, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f381ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b517e6db",
   "metadata": {},
   "source": [
    "### PCA\n",
    "### Apply and create dataframe\n",
    "Split the dataframe into two separate dataframes (feature data and target data (X,y)). Apply PCA to the feature data, build a new dataframe from the transformed components and store it as _X\\_pca_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b84bb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "362cbc52",
   "metadata": {},
   "source": [
    "Plot the heatmap of the PCA-transformed dataframe and compare it to the map from above. How can you explain the differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f9ee28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab26b9bb",
   "metadata": {},
   "source": [
    "Take a look at the explained variance of the principal components and display display it in a plot. How many components would you choose to get the best trade-off between dimensionality-reduction and information loss? How much of the variance in the entire dataset do they represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdbf8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab01d5f8",
   "metadata": {},
   "source": [
    "Now create a dataframe out of your chosen components (_np.hstack()_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a482976",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ed48f2",
   "metadata": {},
   "source": [
    "Create a scatter plot of the first two PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6078ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa14c3b",
   "metadata": {},
   "source": [
    "### Training\n",
    "Split the datasets (original and PCA-transformed) into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27cb5e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f727bc2",
   "metadata": {},
   "source": [
    " Use different algorithms of your choice to try and achieve the highest target score.\n",
    "Use _train\\_test\\_splot from _sklearn.model_selection_ (random_state: 25, test_size: 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ccc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49c0c4",
   "metadata": {},
   "source": [
    "Plot the results. Which of the algorithms you chose performed best? Do the results surprise you in any way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e8332",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8104c95c",
   "metadata": {},
   "source": [
    "## Task 3\n",
    "In this task we try to predict the anticipated final status of bank loans, given various attributes describing the loan taker. The _credit_ dataset for this task can be downloaded from __[here](https://github.com/DataScienceLabFHSWF/machine-learning-book/tree/main/data/pca)__.  \n",
    "As always, load the data into a dataframe and get familiar with it (__[might be useful](https://www.investopedia.com/terms/c/chargeoff.asp)__). What kinds of categories do you see (numerical, categorical)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962842ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d4821f7",
   "metadata": {},
   "source": [
    "Are there any features that correlate strongly with each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bbc37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7baf8b14",
   "metadata": {},
   "source": [
    "### Dealing with missing data\n",
    "You might have noticed that there are quite a few missing/na values in our dataset. Take a closer look at each of the attributes that has missing values and try to decide how to best deal with it (fill with constant value, drop entirely, ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec490ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77574568",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1c9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a201725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea2880f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a669979e",
   "metadata": {},
   "source": [
    "### Categorical variables\n",
    "At this point your dataframe should have no attributes with missing values left. In the next step you should use One-Hot-Encoding or Label-Encoding to transform the categorical values into numerical/binary ones.  \n",
    "What are the names of these features? Which technique(s) did you use?  \n",
    "\n",
    "First encode the target column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162a6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a6f284c",
   "metadata": {},
   "source": [
    "And now the other attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4c6e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851118ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413acf91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6860ac66",
   "metadata": {},
   "source": [
    "Now append these transformed matrizes to your cleaned dataframe (_pd.concat()_) to create the final version of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d71853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8934ef03",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Perform the PCA and plot the important metrics. How many components are needed to explain most of the variance?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c40647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7eb7c1b",
   "metadata": {},
   "source": [
    "### Training\n",
    "Use the cleaned and encoded dataframe for training and test data. Compare the results of PCA-transformed and the cleaned, untransformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc01d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d64b20cc",
   "metadata": {},
   "source": [
    "### Evaluation and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ef63b8",
   "metadata": {},
   "source": [
    "Apply various machine learning algorithms to your training data and evaluate on your test data. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105efa63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ae651f2",
   "metadata": {},
   "source": [
    "Plot your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eba7bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('airflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "568ce6a90abe48cd4c71813e2e18d608b5934a77a079188d46a97b4cb4032653"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
